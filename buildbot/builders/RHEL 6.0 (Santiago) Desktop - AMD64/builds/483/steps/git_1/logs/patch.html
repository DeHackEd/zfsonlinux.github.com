<html>

<!-- Mirrored from pip.chaos:8010/builders/RHEL%206.0%20(Santiago)%20Desktop%20-%20AMD64/builds/483/steps/git_1/logs/patch by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 17 Mar 2012 01:52:49 GMT -->
<head><title>Log File contents</title>

<style type="text/css">
 div.data {
  font-family: "Courier New", courier, monotype;
 }
 span.stdout {
  font-family: "Courier New", courier, monotype;
 }
 span.stderr {
  font-family: "Courier New", courier, monotype;
  color: red;
 }
 span.header {
  font-family: "Courier New", courier, monotype;
  color: blue;
 }
</style>
</head>
<body vlink="#800080">
<a href="patch/texthtml.html">(view as text)</a><br />
<pre>
<span class="stdout">diff --git a/META b/META
index f70acab..be0d25a 100644
--- a/META
+++ b/META
@@ -2,7 +2,7 @@ Meta:         1
 Name:         zfs
 Branch:       1.0
 Version:      0.6.0
-Release:      rc6
+Release:      rc6_10chaos
 Release-Tags: relext
 License:      CDDL
 Author:       Sun Microsystems/Oracle, Lawrence Livermore National Laboratory
diff --git a/module/zfs/arc.c b/module/zfs/arc.c
index 10317b6..dc98be9 100644
--- a/module/zfs/arc.c
+++ b/module/zfs/arc.c
@@ -158,7 +158,10 @@ typedef enum arc_reclaim_strategy {
 } arc_reclaim_strategy_t;
 
 /* number of seconds before growing cache again */
-static int		arc_grow_retry = 60;
+static int		arc_grow_retry = 5;
+
+/* expiration time for arc_no_grow */
+static clock_t		arc_grow_time = 0;
 
 /* shift of arc_c for calculating both min and max arc_p */
 static int		arc_p_min_shift = 4;
@@ -909,21 +912,6 @@ buf_dest(void *vbuf, void *unused)
 	arc_space_return(sizeof (arc_buf_t), ARC_SPACE_HDRS);
 }
 
-/*
- * Reclaim callback -- invoked when memory is low.
- */
-/* ARGSUSED */
-static void
-hdr_recl(void *unused)
-{
-	/*
-	 * umem calls the reclaim func when we destroy the buf cache,
-	 * which is after we do arc_fini().
-	 */
-	if (!arc_dead)
-		cv_signal(&amp;arc_reclaim_thr_cv);
-}
-
 static void
 buf_init(void)
 {
@@ -956,7 +944,7 @@ retry:
 	}
 
 	hdr_cache = kmem_cache_create(&quot;arc_buf_hdr_t&quot;, sizeof (arc_buf_hdr_t),
-	    0, hdr_cons, hdr_dest, hdr_recl, NULL, NULL, 0);
+	    0, hdr_cons, hdr_dest, NULL, NULL, NULL, 0);
 	buf_cache = kmem_cache_create(&quot;arc_buf_t&quot;, sizeof (arc_buf_t),
 	    0, buf_cons, buf_dest, NULL, NULL, NULL, 0);
 
@@ -2100,16 +2088,13 @@ arc_flush(spa_t *spa)
 }
 
 void
-arc_shrink(void)
+arc_shrink(uint64_t bytes)
 {
 	if (arc_c &gt; arc_c_min) {
 		uint64_t to_free;
 
-#ifdef _KERNEL
-		to_free = MAX(arc_c &gt;&gt; arc_shrink_shift, ptob(needfree));
-#else
-		to_free = arc_c &gt;&gt; arc_shrink_shift;
-#endif
+		to_free = bytes ? bytes : arc_c &gt;&gt; arc_shrink_shift;
+
 		if (arc_c &gt; arc_c_min + to_free)
 			atomic_add_64(&amp;arc_c, -to_free);
 		else
@@ -2128,66 +2113,8 @@ arc_shrink(void)
 		arc_adjust();
 }
 
-static int
-arc_reclaim_needed(void)
-{
-#ifdef _KERNEL
-	uint64_t extra;
-
-	if (needfree)
-		return (1);
-
-	/*
-	 * take 'desfree' extra pages, so we reclaim sooner, rather than later
-	 */
-	extra = desfree;
-
-	/*
-	 * check that we're out of range of the pageout scanner.  It starts to
-	 * schedule paging if freemem is less than lotsfree and needfree.
-	 * lotsfree is the high-water mark for pageout, and needfree is the
-	 * number of needed free pages.  We add extra pages here to make sure
-	 * the scanner doesn't start up while we're freeing memory.
-	 */
-	if (freemem &lt; lotsfree + needfree + extra)
-		return (1);
-
-	/*
-	 * check to make sure that swapfs has enough space so that anon
-	 * reservations can still succeed. anon_resvmem() checks that the
-	 * availrmem is greater than swapfs_minfree, and the number of reserved
-	 * swap pages.  We also add a bit of extra here just to prevent
-	 * circumstances from getting really dire.
-	 */
-	if (availrmem &lt; swapfs_minfree + swapfs_reserve + extra)
-		return (1);
-
-#if defined(__i386)
-	/*
-	 * If we're on an i386 platform, it's possible that we'll exhaust the
-	 * kernel heap space before we ever run out of available physical
-	 * memory.  Most checks of the size of the heap_area compare against
-	 * tune.t_minarmem, which is the minimum available real memory that we
-	 * can have in the system.  However, this is generally fixed at 25 pages
-	 * which is so low that it's useless.  In this comparison, we seek to
-	 * calculate the total heap-size, and reclaim if more than 3/4ths of the
-	 * heap is allocated.  (Or, in the calculation, if less than 1/4th is
-	 * free)
-	 */
-	if (btop(vmem_size(heap_arena, VMEM_FREE)) &lt;
-	    (btop(vmem_size(heap_arena, VMEM_FREE | VMEM_ALLOC)) &gt;&gt; 2))
-		return (1);
-#endif
-
-#else
-	if (spa_get_random(100) == 0)
-		return (1);
-#endif
-	return (0);
-}
-
 static void
-arc_kmem_reap_now(arc_reclaim_strategy_t strat)
+arc_kmem_reap_now(arc_reclaim_strategy_t strat, uint64_t bytes)
 {
 	size_t			i;
 	kmem_cache_t		*prev_cache = NULL;
@@ -2200,7 +2127,7 @@ arc_kmem_reap_now(arc_reclaim_strategy_t strat)
 	 * reap free buffers from the arc kmem caches.
 	 */
 	if (strat == ARC_RECLAIM_AGGR)
-		arc_shrink();
+		arc_shrink(bytes);
 
 	for (i = 0; i &lt; SPA_MAXBLOCKSIZE &gt;&gt; SPA_MINBLOCKSHIFT; i++) {
 		if (zio_buf_cache[i] != prev_cache) {
@@ -2220,8 +2147,6 @@ arc_kmem_reap_now(arc_reclaim_strategy_t strat)
 static void
 arc_reclaim_thread(void)
 {
-	clock_t			growtime = 0;
-	arc_reclaim_strategy_t	last_reclaim = ARC_RECLAIM_CONS;
 	callb_cpr_t		cpr;
 	int64_t			prune;
 
@@ -2229,7 +2154,10 @@ arc_reclaim_thread(void)
 
 	mutex_enter(&amp;arc_reclaim_thr_lock);
 	while (arc_thread_exit == 0) {
-		if (arc_reclaim_needed()) {
+#ifndef _KERNEL
+		arc_reclaim_strategy_t	last_reclaim = ARC_RECLAIM_CONS;
+
+		if (spa_get_random(100) == 0) {
 
 			if (arc_no_grow) {
 				if (last_reclaim == ARC_RECLAIM_CONS) {
@@ -2244,14 +2172,16 @@ arc_reclaim_thread(void)
 			}
 
 			/* reset the growth delay for every reclaim */
-			growtime = ddi_get_lbolt() + (arc_grow_retry * hz);
+			arc_grow_time = ddi_get_lbolt()+(arc_grow_retry * hz);
 
-			arc_kmem_reap_now(last_reclaim);
+			arc_kmem_reap_now(last_reclaim, 0);
 			arc_warm = B_TRUE;
+		}
+#endif /* !_KERNEL */
 
-		} else if (arc_no_grow &amp;&amp; ddi_get_lbolt() &gt;= growtime) {
+		/* No recent memory pressure allow the ARC to grow. */
+		if (arc_no_grow &amp;&amp; ddi_get_lbolt() &gt;= arc_grow_time)
 			arc_no_grow = FALSE;
-		}
 
 		/*
 		 * Keep meta data usage within limits, arc_shrink() is not
@@ -2281,29 +2211,30 @@ arc_reclaim_thread(void)
 }
 
 #ifdef _KERNEL
-/*
- * Under Linux the arc shrinker may be called for synchronous (direct)
- * reclaim, or asynchronous (indirect) reclaim.  When called by kswapd
- * for indirect reclaim we take a conservative approach and just reap
- * free slabs from the ARC caches.  If this proves to be insufficient
- * direct reclaim will be trigger.  In direct reclaim a more aggressive
- * strategy is used, data is evicted from the ARC and free slabs reaped.
- */
+static uint64_t
+arc_evictable_memory(void) {
+	uint64_t evictable_memory =
+	    arc_mru-&gt;arcs_lsize[ARC_BUFC_DATA] +
+	    arc_mru-&gt;arcs_lsize[ARC_BUFC_METADATA] +
+	    arc_mfu-&gt;arcs_lsize[ARC_BUFC_DATA] +
+	    arc_mfu-&gt;arcs_lsize[ARC_BUFC_METADATA];
+
+	return MIN(evictable_memory, arc_size - arc_c_min);
+}
+
 static int
 __arc_shrinker_func(struct shrinker *shrink, struct shrink_control *sc)
 {
-	arc_reclaim_strategy_t strategy;
-	int arc_reclaim;
+	uint64_t pages;
 
-	/* Return number of reclaimable pages based on arc_shrink_shift */
-	arc_reclaim = MAX(btop(((int64_t)arc_size - (int64_t)arc_c_min))
-	    &gt;&gt; arc_shrink_shift, 0);
-	if (sc-&gt;nr_to_scan == 0)
-		return (arc_reclaim);
+	/* The arc is considered warm once reclaim has occurred */
+	if (unlikely(arc_warm == B_FALSE))
+		arc_warm = B_TRUE;
 
-	/* Prevent reclaim below arc_c_min */
-	if (arc_reclaim &lt;= 0)
-		return (-1);
+	/* Return the potential number of reclaimable pages */
+	pages = btop(arc_evictable_memory());
+	if (sc-&gt;nr_to_scan == 0)
+		return (pages);
 
 	/* Not allowed to perform filesystem reclaim */
 	if (!(sc-&gt;gfp_mask &amp; __GFP_FS))
@@ -2313,20 +2244,31 @@ __arc_shrinker_func(struct shrinker *shrink, struct shrink_control *sc)
 	if (mutex_tryenter(&amp;arc_reclaim_thr_lock) == 0)
 		return (-1);
 
+	/*
+	 * Evict the requested number of pages by shrinking arc_c the
+	 * requested amount.  If there is nothing left to evict just
+	 * reap whatever we can from the various arc slabs.
+	 */
+	if (pages &gt; 0) {
+		arc_kmem_reap_now(ARC_RECLAIM_AGGR, ptob(sc-&gt;nr_to_scan));
+		pages = btop(arc_evictable_memory());
+	} else {
+		arc_kmem_reap_now(ARC_RECLAIM_CONS, ptob(sc-&gt;nr_to_scan));
+		pages = -1;
+	}
+
+	arc_no_grow = TRUE;
+	arc_grow_time = ddi_get_lbolt() + (arc_grow_retry * hz);
+
 	if (current_is_kswapd()) {
-		strategy = ARC_RECLAIM_CONS;
-		ARCSTAT_INCR(arcstat_memory_indirect_count, 1);
+		ARCSTAT_BUMP(arcstat_memory_indirect_count);
 	} else {
-		strategy = ARC_RECLAIM_AGGR;
-		ARCSTAT_INCR(arcstat_memory_direct_count, 1);
+		ARCSTAT_BUMP(arcstat_memory_direct_count);
 	}
 
-	arc_kmem_reap_now(strategy);
-	arc_reclaim = MAX(btop(((int64_t)arc_size - (int64_t)arc_c_min))
-	    &gt;&gt; arc_shrink_shift, 0);
 	mutex_exit(&amp;arc_reclaim_thr_lock);
 
-	return (arc_reclaim);
+	return (pages);
 }
 SPL_SHRINKER_CALLBACK_WRAPPER(arc_shrinker_func);
 
@@ -2374,11 +2316,6 @@ arc_adapt(int bytes, arc_state_t *state)
 	}
 	ASSERT((int64_t)arc_p &gt;= 0);
 
-	if (arc_reclaim_needed()) {
-		cv_signal(&amp;arc_reclaim_thr_cv);
-		return;
-	}
-
 	if (arc_no_grow)
 		return;
 
@@ -2423,7 +2360,7 @@ arc_evict_needed(arc_buf_contents_t type)
 		return (1);
 #endif
 
-	if (arc_reclaim_needed())
+	if (arc_no_grow)
 		return (1);
 
 	return (arc_size &gt; arc_c);
@@ -3556,48 +3493,22 @@ static int
 arc_memory_throttle(uint64_t reserve, uint64_t inflight_data, uint64_t txg)
 {
 #ifdef _KERNEL
-	uint64_t available_memory = ptob(freemem);
-	static uint64_t page_load = 0;
-	static uint64_t last_txg = 0;
+	uint64_t available_memory = ptob(nr_free_pages());
 
 #if defined(__i386)
 	available_memory =
 	    MIN(available_memory, vmem_size(heap_arena, VMEM_FREE));
 #endif
+	available_memory += arc_evictable_memory();
+
 	if (available_memory &gt;= zfs_write_limit_max)
 		return (0);
 
-	if (txg &gt; last_txg) {
-		last_txg = txg;
-		page_load = 0;
-	}
-	/*
-	 * If we are in pageout, we know that memory is already tight,
-	 * the arc is already going to be evicting, so we just want to
-	 * continue to let page writes occur as quickly as possible.
-	 */
-	if (curproc == proc_pageout) {
-		if (page_load &gt; MAX(ptob(minfree), available_memory) / 4)
-			return (ERESTART);
-		/* Note: reserve is inflated, so we deflate */
-		page_load += reserve / 8;
-		return (0);
-	} else if (page_load &gt; 0 &amp;&amp; arc_reclaim_needed()) {
-		/* memory is low, del</span><span class="stdout">ay before restarting */
+	if (arc_no_grow) {
 		ARCSTAT_INCR(arcstat_memory_throttle_count, 1);
 		DMU_TX_STAT_BUMP(dmu_tx_memory_reclaim);
 		return (EAGAIN);
 	}
-	page_load = 0;
-
-	if (arc_size &gt; arc_c_min) {
-		uint64_t evictable_memory =
-		    arc_mru-&gt;arcs_lsize[ARC_BUFC_DATA] +
-		    arc_mru-&gt;arcs_lsize[ARC_BUFC_METADATA] +
-		    arc_mfu-&gt;arcs_lsize[ARC_BUFC_DATA] +
-		    arc_mfu-&gt;arcs_lsize[ARC_BUFC_METADATA];
-		available_memory += MIN(evictable_memory, arc_size - arc_c_min);
-	}
 
 	if (inflight_data &gt; available_memory / 4) {
 		ARCSTAT_INCR(arcstat_memory_throttle_count, 1);
@@ -4773,7 +4684,7 @@ l2arc_feed_thread(void)
 		/*
 		 * Avoid contributing to memory pressure.
 		 */
-		if (arc_reclaim_needed()) {
+		if (arc_no_grow) {
 			ARCSTAT_BUMP(arcstat_l2_abort_lowmem);
 			spa_config_exit(spa, SCL_L2ARC, dev);
 			continue;
diff --git a/module/zfs/dmu_tx.c b/module/zfs/dmu_tx.c
index 17eb527..22af4dd 100644
--- a/module/zfs/dmu_tx.c
+++ b/module/zfs/dmu_tx.c
@@ -825,7 +825,8 @@ dmu_tx_dirty_buf(dmu_tx_t *tx, dmu_buf_impl_t *db)
 
 	for (txh = list_head(&amp;tx-&gt;tx_holds); txh;
 	    txh = list_next(&amp;tx-&gt;tx_holds, txh)) {
-		ASSERT(dn == NULL || dn-&gt;dn_assigned_txg == tx-&gt;tx_txg);
+		ASSERT3P(dn, ==, NULL);
+		ASSERT3U(dn-&gt;dn_assigned_txg, ==, tx-&gt;tx_txg);
 		if (txh-&gt;txh_dnode == dn &amp;&amp; txh-&gt;txh_type != THT_NEWOBJECT)
 			match_object = TRUE;
 		if (txh-&gt;txh_dnode == NULL || txh-&gt;txh_dnode == dn) {
</span></pre>
</body>
<!-- Mirrored from pip.chaos:8010/builders/RHEL%206.0%20(Santiago)%20Desktop%20-%20AMD64/builds/483/steps/git_1/logs/patch by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 17 Mar 2012 01:52:51 GMT -->
</html>
